{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metapath-visits HGCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "from sklearn.model_selection import train_test_split\n",
    "import networkx as nx \n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "filename = \"Data/symmetricPath\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the metapath (pathsim-based) overall similarity matrix\n",
    "with open(f'{filename}/data.pickle', 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "\n",
    "# Reading the nodes of the similarity matrix\n",
    "with open(f'{filename}/patients.pickle', 'rb') as file:\n",
    "    Patients = pickle.load(file)\n",
    "\n",
    "with open(f'{filename}/visits.pickle', 'rb') as file:\n",
    "    visits = pickle.load(file)\n",
    "\n",
    "with open(f'{filename}/medication.pickle', 'rb') as file:\n",
    "    Medications = pickle.load(file)\n",
    "\n",
    "with open(f'{filename}/diagnosis.pickle', 'rb') as file:\n",
    "    Diagnosis = pickle.load(file)\n",
    "\n",
    "with open(f'{filename}/procedures.pickle', 'rb') as file:\n",
    "    Procedures = pickle.load(file)\n",
    "\n",
    "with open(f'{filename}/visitsLOS.pickle', 'rb') as file:\n",
    "    visitsLOS = pickle.load(file)\n",
    "\n",
    "# Reading Length of Stay on the visits.\n",
    "newVisitsLOS = {v: visitsLOS[v] for v in visits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "someK = 99999\n",
    "\n",
    "# Setting the graph based on the similarity matrix\n",
    "G = nx.Graph()\n",
    "\n",
    "Nodes = Patients + visits + Medications + Diagnosis  + Procedures\n",
    "G.add_nodes_from(Nodes)\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "for node in G.nodes:\n",
    "    if node[0]=='V':\n",
    "        G.nodes[node]['y'] = newVisitsLOS[node]\n",
    "    else:\n",
    "        G.nodes[node]['y'] = someK\n",
    "\n",
    "degrees = [G.nodes[node]['y'] for node in G.nodes if G.nodes[node]['y']!=someK]\n",
    "plt.hist(degrees, bins=100, edgecolor='k')  \n",
    "plt.xlabel('LOS in Days')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Degree Distribution')\n",
    "plt.show()\n",
    "# ------------------------------------------------------------------------\n",
    "# splitting the LOS into binary labels. \n",
    "a,b,c = 0,0,0\n",
    "L, H = 1, 5\n",
    "for node in G.nodes:\n",
    "    if node[0]=='V':\n",
    "        if G.nodes[node]['y']<=L:\n",
    "            G.nodes[node]['y']=0\n",
    "            a+=1\n",
    "\n",
    "        elif G.nodes[node]['y']>L and G.nodes[node]['y']<H:\n",
    "            G.nodes[node]['y']=1\n",
    "            b+=1\n",
    "\n",
    "        elif G.nodes[node]['y']>=H:\n",
    "            G.nodes[node]['y']=someK\n",
    "            c+=1\n",
    "\n",
    "print(f'Less than {L} days = {a}\\nMore than {H} days = {c}\\n{L}<LOS<{H} = {b}')\n",
    "\n",
    "# ------------------------------------------------------------------------# Plotting the frequency of LOS...\n",
    "degrees = [G.nodes[node]['y'] for node in G.nodes if G.nodes[node]['y']!=someK]\n",
    "plt.hist(degrees, bins=100, edgecolor='k')  \n",
    "plt.xlabel('LOS in Days')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Degree Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the weight of the edges into the graph\n",
    "for i, v in enumerate(Nodes):\n",
    "    for j, u in enumerate(Nodes):\n",
    "        if data[i][j]>0:\n",
    "            G.add_edge(v, u, weight=data[i][j])\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# X = np.random.randn(len(list(G.nodes())), 128)\n",
    "X = np.eye(len(list(G.nodes())))\n",
    "\n",
    "for i, node in enumerate(list(G.nodes())):\n",
    "    G.nodes[node]['x'] = X[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the data structure\n",
    "data = from_networkx(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newY = []\n",
    "\n",
    "# extracting the visits nodes and their indexes for the learning process.\n",
    "for i,v in enumerate(list(data.y)):\n",
    "    if v!=someK:\n",
    "        newY.append([i,v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, y_left = train_test_split(newY    , train_size=0.7, random_state=42)\n",
    "y_val,   y_test = train_test_split(y_left  , test_size=2/3 , random_state=42)\n",
    "\n",
    "# Create masks for train, validation, and test sets\n",
    "train_mask = np.zeros(len(data.y), dtype=bool)\n",
    "val_mask   = np.zeros(len(data.y), dtype=bool)\n",
    "test_mask  = np.zeros(len(data.y), dtype=bool)\n",
    "\n",
    "for i,v in y_train:\n",
    "    train_mask[i] = True\n",
    "\n",
    "for i,v in y_val:\n",
    "    val_mask[i] = True\n",
    "\n",
    "for i, v in y_test:\n",
    "    test_mask[i] = True\n",
    "\n",
    "print(train_mask.shape)\n",
    "print(val_mask.shape)\n",
    "print(test_mask.shape)\n",
    "\n",
    "total = len(newY)\n",
    "print('Train = ', sum([1 if v else 0 for v in train_mask])/total)\n",
    "print('Val = ',   sum([1 if v else 0 for v in val_mask])  /total)\n",
    "print('Test = ',  sum([1 if v else 0 for v in test_mask]) /total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.train_mask = train_mask\n",
    "data.test_mask  = test_mask\n",
    "data.val_mask   = val_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GNNModel as GNNM\n",
    "import torch\n",
    "\n",
    "data.x = data.x.float()\n",
    "data.y = data.y.long()\n",
    "num_classes = 2\n",
    "\n",
    "# Check if a GPU is available, otherwise use CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "data = data.to(device)\n",
    "gcn = GNNM.GCN3(data.num_node_features, num_classes).to(device)\n",
    "\n",
    "optimizer_gcn = torch.optim.Adam(gcn.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "gcn, df = GNNM.train_node_classifier(gcn, data, optimizer_gcn, criterion, n_epochs=5)\n",
    "\n",
    "GNNM.plt_performance(df)\n",
    "# ----------------------------------------------------------------------------\n",
    "pred = gcn(data).argmax(dim=1)\n",
    "\n",
    "pred    = pred  [data.test_mask].detach().cpu().numpy()\n",
    "correct = data.y[data.test_mask].detach().cpu().numpy()\n",
    "\n",
    "GNNM.create_confusion_matrix(pred, correct)\n",
    "\n",
    "\n",
    "test_acc, _ = GNNM.eval_node_classifier(gcn, data, data.test_mask)\n",
    "print(f'Test Acc: {test_acc:.3f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import GNNModel as GNNM\n",
    "# import torch\n",
    "\n",
    "# data.x = data.x.float()\n",
    "# data.y = data.y.float()\n",
    "\n",
    "# # Check if a GPU is available, otherwise use CPU\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# data = data.to(device)\n",
    "# gcn = GNNM.GCNRegression(data.num_node_features).to(device)\n",
    "\n",
    "# optimizer_gcn = torch.optim.Adam(gcn.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "# # criterion = torch.nn.CrossEntropyLoss()\n",
    "# criterion = nn.MSELoss()\n",
    "# gcn, df = GNNM.train_node_classifier(gcn, data, optimizer_gcn, criterion, n_epochs=25)\n",
    "# # ----------------------------------------------------------------------------\n",
    "\n",
    "# pred = gcn(data).argmax(dim=1)\n",
    "\n",
    "# pred = pred[data.test_mask] .detach().cpu().numpy()\n",
    "# correct = data.y[data.test_mask].detach().cpu().numpy()\n",
    "\n",
    "# [(pred[i], correct[i]) for i in range(len(correct))]\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Assuming you have already defined 'pred' and 'correct'\n",
    "\n",
    "# # Create a scatter plot\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.scatter(correct, pred, c='blue', alpha=0.6, edgecolors='k')\n",
    "\n",
    "# # Add labels and title\n",
    "# plt.xlabel('Actual Values (correct)')\n",
    "# plt.ylabel('Predicted Values (pred)')\n",
    "# plt.title('Predicted vs. Actual Values')\n",
    "\n",
    "# # Add a diagonal line for reference (perfect prediction)\n",
    "# plt.plot([min(correct), max(correct)], [min(correct), max(correct)], linestyle='--', color='red', linewidth=2)\n",
    "# plt.legend()\n",
    "# # Show the plot\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "# import numpy as np\n",
    "\n",
    "# # Actual target values (ground truth)\n",
    "# actual_values = correct\n",
    "\n",
    "# # Predicted values from your regression model\n",
    "# predicted_values = pred\n",
    "\n",
    "# # Calculate and print Mean Absolute Error (MAE)\n",
    "# mae = mean_absolute_error(actual_values, predicted_values)\n",
    "# print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "\n",
    "# # Calculate and print Mean Squared Error (MSE)\n",
    "# mse = mean_squared_error(actual_values, predicted_values)\n",
    "# print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "\n",
    "# # Calculate and print Root Mean Squared Error (RMSE)\n",
    "# rmse = np.sqrt(mse)\n",
    "# print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "\n",
    "# # Calculate and print R-squared (R²)\n",
    "# r_squared = r2_score(actual_values, predicted_values)\n",
    "# print(f\"R-squared (R²): {r_squared:.2f}\")\n",
    "\n",
    "\n",
    "# # GNNM.create_confusion_matrix(pred, correct)\n",
    "\n",
    "\n",
    "# # test_acc, _ = GNNM.eval_node_classifier(gcn, data, data.test_mask)\n",
    "# # print(f'Test Acc: {test_acc:.3f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
