{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNN on MIMIC-III EHR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "# building the dataset\n",
    "# Go to Pratip/Deep Learning Tutorial/GNN/PSG_GCN.ipynb\n",
    "import os\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import networkx as nx \n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)\n",
    "use_cuda_if_available = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'Data/MIMIC resources'\n",
    "\n",
    "df_Admissions = pd.read_csv(f'{folder_path}/ADMISSIONS.csv')\n",
    "\n",
    "df_Patients = pd.read_csv(f'{folder_path}/PATIENTS.csv')\n",
    "\n",
    "# medication!\n",
    "df_Prescription = pd.read_csv(f'{folder_path}/PRESCRIPTIONS.csv')\n",
    "\n",
    "# Diagnosis!\n",
    "df_DiagnosisICD = pd.read_csv(f'{folder_path}/DIAGNOSES_ICD.csv')\n",
    "\n",
    "# Procedures!\n",
    "df_ProceduresICD = pd.read_csv(f'{folder_path}/PROCEDURES_ICD.csv')\n",
    "# ICUStays\n",
    "df_Icustays = pd.read_csv(f'{folder_path}/ICUSTAYS.csv')\n",
    "\n",
    "\n",
    "df_ProceduresICD.dropna(subset=['ICD9_CODE'], inplace=True)\n",
    "df_Prescription.dropna(subset=['drug'], inplace=True)\n",
    "df_DiagnosisICD.dropna(subset=['ICD9_CODE'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>SEQ_NUM</th>\n",
       "      <th>ICD9_CODE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>944</td>\n",
       "      <td>62641</td>\n",
       "      <td>154460</td>\n",
       "      <td>3</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>945</td>\n",
       "      <td>2592</td>\n",
       "      <td>130856</td>\n",
       "      <td>1</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>946</td>\n",
       "      <td>2592</td>\n",
       "      <td>130856</td>\n",
       "      <td>2</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>947</td>\n",
       "      <td>55357</td>\n",
       "      <td>119355</td>\n",
       "      <td>1</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>948</td>\n",
       "      <td>55357</td>\n",
       "      <td>119355</td>\n",
       "      <td>2</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240090</th>\n",
       "      <td>228330</td>\n",
       "      <td>67415</td>\n",
       "      <td>150871</td>\n",
       "      <td>5</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240091</th>\n",
       "      <td>228331</td>\n",
       "      <td>67415</td>\n",
       "      <td>150871</td>\n",
       "      <td>6</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240092</th>\n",
       "      <td>228332</td>\n",
       "      <td>67415</td>\n",
       "      <td>150871</td>\n",
       "      <td>7</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240093</th>\n",
       "      <td>228333</td>\n",
       "      <td>67415</td>\n",
       "      <td>150871</td>\n",
       "      <td>8</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240094</th>\n",
       "      <td>228334</td>\n",
       "      <td>67415</td>\n",
       "      <td>150871</td>\n",
       "      <td>9</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240095 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ROW_ID  SUBJECT_ID  HADM_ID  SEQ_NUM ICD9_CODE\n",
       "0          944       62641   154460        3        34\n",
       "1          945        2592   130856        1        96\n",
       "2          946        2592   130856        2        38\n",
       "3          947       55357   119355        1        96\n",
       "4          948       55357   119355        2        33\n",
       "...        ...         ...      ...      ...       ...\n",
       "240090  228330       67415   150871        5        37\n",
       "240091  228331       67415   150871        6        38\n",
       "240092  228332       67415   150871        7        88\n",
       "240093  228333       67415   150871        8        38\n",
       "240094  228334       67415   150871        9        37\n",
       "\n",
       "[240095 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert all procedure codes into two digits only\n",
    "# convert all diagnosis codes into three digits only\n",
    "\n",
    "def extract3(code):\n",
    "    return str(code)[:3]\n",
    "def extract2(code):\n",
    "    return str(code)[:2]\n",
    "\n",
    "df_DiagnosisICD['ICD9_CODE'] = df_DiagnosisICD['ICD9_CODE'].apply(extract3)\n",
    "df_ProceduresICD['ICD9_CODE'] = df_ProceduresICD['ICD9_CODE'].apply(extract2)\n",
    "\n",
    "df_ProceduresICD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Patients = 46520\n",
      "Number of Admissions = 58976\n",
      "Number of Diagnosis = 942\n",
      "Number of procedures = 89\n",
      "Number of Medication = 592\n"
     ]
    }
   ],
   "source": [
    "Procedures = sorted(df_ProceduresICD['ICD9_CODE'].unique())\n",
    "Medication = sorted(df_Prescription['drug'].unique())\n",
    "Diagnosis  = df_DiagnosisICD['ICD9_CODE'].unique()\n",
    "Patients = df_Patients['SUBJECT_ID'].unique()\n",
    "Admissions = df_Admissions['HADM_ID'].unique()\n",
    "\n",
    "print(f'Number of Patients = {len(Patients)}')\n",
    "\n",
    "print(f'Number of Admissions = {len(Admissions)}')\n",
    "\n",
    "print(f'Number of Diagnosis = {len(Diagnosis)}')\n",
    "\n",
    "print(f'Number of procedures = {len(Procedures)}')\n",
    "\n",
    "print(f'Number of Medication = {len(Medication)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LUNG Patients only\n",
    "**ICD9=162**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        ROW_ID  SUBJECT_ID  HADM_ID  SEQ_NUM ICD9_CODE\n",
      "584        813         103   130744      3.0       162\n",
      "592        821         103   133550      3.0       162\n",
      "1037       430          56   181711      2.0       162\n",
      "1936      3472         291   126219      3.0       162\n",
      "2541      1882         150   108732      1.0       162\n",
      "...        ...         ...      ...      ...       ...\n",
      "649252  638905       97301   195551      3.0       162\n",
      "650147  640459       97603   166108      1.0       162\n",
      "650674  632564       96004   141589      3.0       162\n",
      "650784  632674       96023   116385      6.0       162\n",
      "650789  632679       96023   174400      1.0       162\n",
      "\n",
      "[1181 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# # restricting to LUNG disease\n",
    "ICD_Diagnosis_Lung = [i for i in Diagnosis if str(i).startswith('162')]\n",
    "\n",
    "df_sub = df_DiagnosisICD[df_DiagnosisICD['ICD9_CODE'].str.startswith('162')]\n",
    "\n",
    "\n",
    "# Taking the entire dataset!\n",
    "# df_sub = df_DiagnosisICD.copy()\n",
    "# Display the sub dataframe\n",
    "print(df_sub)\n",
    "\n",
    "df_sub.to_csv('Data/LungPatients.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Patients = 998\n",
      "Number of Admission = 1171\n"
     ]
    }
   ],
   "source": [
    "Patients = df_sub['SUBJECT_ID'].unique()\n",
    "Admissions = df_sub['HADM_ID'].unique()\n",
    "\n",
    "print(f'Number of Patients = {len(Patients)}')\n",
    "print(f'Number of Admission = {len(Admissions)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting visits (admissions) of each patient sorted by its date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDict(A, DF, label1, label2):\n",
    "    # Extracting the list of NODES for each A item. \n",
    "    # Filter the dataframe to extract Nodes associated with the specified label1\n",
    "    # Extract the list of Nodes associated with the patient\n",
    "    D = {}\n",
    "    for v in A:\n",
    "        df = DF[DF[label1] == v]\n",
    "        id_list = df[label2].tolist()\n",
    "        D[v] = id_list\n",
    "    \n",
    "    return D\n",
    "\n",
    "# def getNodes_and_Edges(D):\n",
    "#     DNodes, DEdges = [], []\n",
    "#     for i, v in D.items():\n",
    "#         for j in v:\n",
    "#             if isinstance(j, (int, float, str)) and not np.isnan(j):\n",
    "#                 DEdges.append([i,j])\n",
    "#                 if j not in DNodes:\n",
    "#                     DNodes.append(j)\n",
    "#     return DNodes, DEdges\n",
    "\n",
    "def getNodes_and_Edges(D):\n",
    "    DEdges = []\n",
    "    DNodes = []\n",
    "    for i, v in D.items():\n",
    "        for j in v:\n",
    "            if isinstance(j, (int, float)) and not np.isnan(j):\n",
    "                DEdges.append([i, j])\n",
    "                if j not in DNodes:\n",
    "                    DNodes.append(j)\n",
    "            elif isinstance(j, str):\n",
    "                # Check if it's a valid numeric string before treating it as a node\n",
    "                try:\n",
    "                    numeric_value = float(j)\n",
    "                    DEdges.append([i, numeric_value])\n",
    "                    if numeric_value not in DNodes:\n",
    "                        DNodes.append(numeric_value)\n",
    "                except ValueError:\n",
    "                    # If it's not a valid numeric string, treat it as a regular string node\n",
    "                    DEdges.append([i, j])\n",
    "                    if j not in DNodes:\n",
    "                        DNodes.append(j)\n",
    "    return DNodes, DEdges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of patient-visit = 1436\n",
      "Total number of Visit-Diagnosis = 18144\n",
      "Total number of Visit-Procedure = 6297\n",
      "Total number of Visit-Medication = 764\n"
     ]
    }
   ],
   "source": [
    "VisitDict      = getDict(Patients, df_Admissions, 'SUBJECT_ID', 'HADM_ID')\n",
    "\n",
    "VisitNodes, PatientVisit = getNodes_and_Edges(VisitDict)\n",
    "print(f'Total number of patient-visit = {len(PatientVisit)}')\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "DiagnosisDict  = getDict(VisitNodes, df_DiagnosisICD, 'HADM_ID', 'ICD9_CODE')\n",
    "DiagnosisNodes, VisitDiagnosis = getNodes_and_Edges(DiagnosisDict)\n",
    "print(f'Total number of Visit-Diagnosis = {len(VisitDiagnosis)}')\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "ProcedureDict  = getDict(VisitNodes, df_ProceduresICD, 'HADM_ID', 'ICD9_CODE')\n",
    "ProcedureNodes, VisitProcedure = getNodes_and_Edges(ProcedureDict)\n",
    "print(f'Total number of Visit-Procedure = {len(VisitProcedure)}')\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "MedicationDict = getDict(VisitNodes, df_Prescription, 'hadm_id', 'drug')\n",
    "MedicationNodes, VisitMedication = getNodes_and_Edges(MedicationDict)\n",
    "print(f'Total number of Visit-Medication = {len(VisitMedication)}')\n",
    "\n",
    "\n",
    "# # -----------------------------------------------------------------------------\n",
    "# ICUSTAYDict = getDict(VisitNodes, df_Prescription, 'hadm_id', 'icustay_id')\n",
    "# ICUSTAYNodes, VisitICUSTAY = getNodes_and_Edges(ICUSTAYDict)\n",
    "# print(f'Total number of Visit-ICUSTAY = {len(VisitICUSTAY)}')\n",
    "\n",
    "# # -----------------------------------------------------------------------------\n",
    "# ICUSTAY_MedicationDict = getDict(ICUSTAYNodes, df_Prescription, 'icustay_id', 'drug')\n",
    "# _, ICUSTAY_Medication = getNodes_and_Edges(ICUSTAY_MedicationDict)\n",
    "# print(f'Total number of ICUSTAY-Medication = {len(ICUSTAY_Medication)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping function for Nodes and edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c, C : Patients\n",
    "# v, V : visits\n",
    "# d, D : Diagnosis\n",
    "# p, P : Procedure\n",
    "# m, M : Medication\n",
    "# i, I : ICUSTAY\n",
    "\n",
    "CV_edges = [[f'C_{u}', f'V_{v}'] for u,v in PatientVisit]\n",
    "VD_edges = [[f'V_{u}', f'D_{v}'] for u,v in VisitDiagnosis]\n",
    "VP_edges = [[f'V_{u}', f'P_{v}'] for u,v in VisitProcedure]\n",
    "VM_edges = [[f'V_{u}', f'M_{v}'] for u,v in VisitMedication]\n",
    "# VI_edges = [[f'V_{u}', f'I_{v}'] for u,v in VisitICUSTAY]\n",
    "# IM_edges = [[f'I_{u}', f'M_{v}'] for u,v in ICUSTAY_Medication]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3107\n"
     ]
    }
   ],
   "source": [
    "edge_index = CV_edges + VD_edges + VP_edges + VM_edges #+ VI_edges\n",
    "\n",
    "tempG = nx.Graph()\n",
    "tempG.add_edges_from(edge_index)\n",
    "\n",
    "print(len(tempG.nodes()))\n",
    "Total_number_of_nodes = len(VisitNodes) + len(DiagnosisNodes) + len(ProcedureNodes) + len(MedicationNodes) + len(Patients)\n",
    "\n",
    "Nodes = list(tempG.nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class1 = df_Admissions[df_Admissions['SUBJECT_ID'].isin(Patients)]\n",
    "\n",
    "dead_patients = class1[class1['HOSPITAL_EXPIRE_FLAG'] == 1]['SUBJECT_ID'].tolist()\n",
    "dead_patients = [f'C_{i}' for i in dead_patients]\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "class2 = df_Icustays[df_Icustays['SUBJECT_ID'].isin(Patients)]\n",
    "\n",
    "new_df = class2.groupby('SUBJECT_ID')['LOS'].sum().reset_index()\n",
    "ICU = getDict(Patients, new_df, 'SUBJECT_ID', 'LOS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ICU2 = {f'C_{i}': sum(v) for i, v in ICU.items()}\n",
    "# Y2 = {}\n",
    "# for i in Nodes:\n",
    "#     if i[0]=='C':\n",
    "#         if ICU2[i]<10:\n",
    "#             Y2[i] = 0\n",
    "#         else:\n",
    "#             Y2[i] = 1\n",
    "#     else:\n",
    "#         Y2[i] = 2\n",
    "\n",
    "# Y = Y2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the nodes, edges and related data to Data \n",
    "\n",
    "https://stackoverflow.com/questions/70452465/how-to-load-in-graph-from-networkx-into-pytorch-geometric-and-set-node-features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = {}\n",
    "for i in Nodes:\n",
    "    if i[0]=='C':\n",
    "        if i in dead_patients:\n",
    "            Y[i] = 1\n",
    "        else:\n",
    "            Y[i] = 0\n",
    "    else:\n",
    "        Y[i] = 2\n",
    "\n",
    "\n",
    "# 0 for alive patient\n",
    "# 1 for dead patient\n",
    "# 2 for other nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\envs\\GNNenv\\Lib\\site-packages\\torch_geometric\\utils\\convert.py:249: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:248.)\n",
      "  data[key] = torch.tensor(value)\n"
     ]
    }
   ],
   "source": [
    "X = np.random.randn(Total_number_of_nodes, 128)\n",
    "# X = np.eye(Total_number_of_nodes)\n",
    "\n",
    "num_classes = 2\n",
    "num_features = X.shape[1]\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "G.add_nodes_from([\n",
    "    (v, {'y': Y[v], 'x': X[i]}) for i, v in enumerate(list(tempG.nodes()))\n",
    "    ])\n",
    "\n",
    "G.add_edges_from(edge_index)\n",
    "\n",
    "data = from_networkx(G)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3107]) 998\n"
     ]
    }
   ],
   "source": [
    "mask_0_1 = (data.y == 0) | (data.y == 1)\n",
    "print(mask_0_1.shape, sum([1 if v else 0 for v in mask_0_1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newY = []\n",
    "# for i, v in enumerate(data.y):\n",
    "#     if v!=2:\n",
    "#         newY.append([i, v])\n",
    "\n",
    "# # Shuffle the data to ensure randomness\n",
    "# np.random.shuffle(newY)\n",
    "\n",
    "# # Calculate the indices for splitting\n",
    "# total_samples = len(newY)\n",
    "# train_samples = int(0.7 * total_samples)\n",
    "# test_samples = int(0.2 * total_samples)\n",
    "\n",
    "# # Split the data into train, test, and validation sets\n",
    "# train_data = newY[:train_samples]\n",
    "# test_data = newY[train_samples:train_samples + test_samples]\n",
    "# val_data = newY[train_samples + test_samples:]\n",
    "\n",
    "# print(len(train_data), len(test_data), len(val_data))\n",
    "\n",
    "# train_mask = torch.tensor([2] * len(data.y))\n",
    "# for i, v in train_data:\n",
    "#     train_mask[i] = v\n",
    "\n",
    "\n",
    "# test_mask = torch.tensor([2] * len(data.y))\n",
    "# for i, v in test_data:\n",
    "#     test_mask[i] = v\n",
    "\n",
    "\n",
    "# val_mask = torch.tensor([2] * len(data.y))\n",
    "# for i, v in val_data:\n",
    "#     val_mask[i] = v\n",
    "\n",
    "# data.train_mask = torch.tensor([train_mask[i]==data.y[i] for i in range(len(data.y))])\n",
    "\n",
    "# data.test_mask = torch.tensor([test_mask[i]==data.y[i] for i in range(len(data.y))])\n",
    "\n",
    "# data.val_mask = torch.tensor([val_mask[i]==data.y[i] for i in range(len(data.y))])\n",
    "# data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch_geometric.transforms as T\n",
    "# from torch_geometric.data import DataLoader, Data, Dataset\n",
    "\n",
    "# from torch.utils.data.dataset import random_split\n",
    "\n",
    "# # Assuming you have a PyTorch Geometric Data object `data` with features (data.x) and labels (data.y)\n",
    "\n",
    "# # Create a mask to filter samples with class labels 0 and 1, and ignore class 2\n",
    "# mask_0_1 = (data.y == 0) | (data.y == 1)\n",
    "# data_filtered = data.x[mask_0_1]\n",
    "# print(len(data_filtered))\n",
    "# data_filtered.edge_index = data.edge_index\n",
    "\n",
    "# # Create a custom dataset class for the filtered data\n",
    "# class FilteredDataset(Dataset):\n",
    "#     def __init__(self, data):\n",
    "#         self.data = data\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         return self.data[idx]\n",
    "\n",
    "# # Convert the filtered data to a custom dataset\n",
    "# filtered_dataset = FilteredDataset(data_filtered)\n",
    "\n",
    "# # Define the sizes for the validation and test sets\n",
    "# num_data = len(filtered_dataset)\n",
    "# num_val = int(0.1 * num_data)\n",
    "# num_test = int(0.2 * num_data)\n",
    "\n",
    "# # Use random_split to split the dataset into train, val, and test sets\n",
    "# train_data, val_data, test_data = random_split(filtered_dataset, [num_data - num_val - num_test, num_val, num_test])\n",
    "\n",
    "# # Now you have the training, validation, and test data objects containing samples with class labels 0 and 1, and ignoring class 2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating new masks out of patient nodes only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.y\n",
    "newY = []\n",
    "for i,v in enumerate(list(data.y)):\n",
    "    if v in [0,1]:\n",
    "        newY.append([i,v])\n",
    "\n",
    "y_train, y_left = train_test_split(newY, train_size=0.2, random_state=42)\n",
    "y_val,   y_test = train_test_split(y_left    , test_size=0.4 , random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199 479 320\n",
      "torch.Size([998])\n",
      "(3107,)\n",
      "0\n",
      "0\n",
      "0\n",
      "(3107,)\n",
      "998\n",
      "998\n",
      "998\n"
     ]
    }
   ],
   "source": [
    "x_filtered = data.x[mask_0_1]\n",
    "y_filtered = data.y[mask_0_1]\n",
    "\n",
    "x_train, x_left, y_train, y_left = train_test_split(x_filtered, y_filtered, train_size=0.2, random_state=42)\n",
    "x_val,   x_test, y_val,   y_test = train_test_split(x_left    , y_left    , test_size=0.4 , random_state=42)\n",
    "\n",
    "print(len(y_train), len(y_val), len(y_test))\n",
    "\n",
    "print(y_filtered.shape)\n",
    "\n",
    "# Create masks for train, validation, and test sets\n",
    "train_mask = np.zeros(len(data.y), dtype=bool)\n",
    "val_mask = np.zeros(len(data.y), dtype=bool)\n",
    "test_mask = np.zeros(len(data.y), dtype=bool)\n",
    "\n",
    "print(train_mask.shape)\n",
    "print(sum([1 if v else 0 for v in train_mask]))\n",
    "print(sum([1 if v else 0 for v in val_mask]))\n",
    "print(sum([1 if v else 0 for v in test_mask]))\n",
    "\n",
    "train_indices = np.where(mask_0_1)[0][np.isin(y_filtered, y_train)]\n",
    "val_indices = np.where(mask_0_1)[0][np.isin(y_filtered, y_val)]\n",
    "test_indices = np.where(mask_0_1)[0][np.isin(y_filtered, y_test)]\n",
    "\n",
    "train_mask[train_indices] = True\n",
    "val_mask[val_indices] = True\n",
    "test_mask[test_indices] = True\n",
    "\n",
    "print(train_mask.shape)\n",
    "print(sum([1 if v else 0 for v in train_mask]))\n",
    "print(sum([1 if v else 0 for v in val_mask]))\n",
    "print(sum([1 if v else 0 for v in test_mask]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.train_mask = train_mask\n",
    "# data.test_mask  = test_mask\n",
    "# data.val_mask   = val_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch.nn import Linear, ReLU\n",
    "# from torch_geometric.nn import SAGEConv\n",
    "# import torch.nn.functional as F\n",
    "# from torch_geometric.nn import GCNConv\n",
    "\n",
    "# class GCN(torch.nn.Module):\n",
    "#     def __init__(self, hidden_channel=16):\n",
    "#         super(GCN, self).__init__()\n",
    "#         print(data.num_node_features)\n",
    "#         self.conv1 = GCNConv(data.num_node_features, hidden_channel, aggr=\"mean\")\n",
    "#         self.conv2 = GCNConv(hidden_channel, 4, aggr=\"mean\")\n",
    "#         # self.conv3 = GCNConv(4, 4)\n",
    "#         self.classifier = Linear(4, num_classes)\n",
    "\n",
    "#     def forward(self, data):\n",
    "#         x, edge_index = data.x, data.edge_index\n",
    "\n",
    "#         x = self.conv1(x, edge_index)\n",
    "#         x = F.relu(x)\n",
    "#         x = self.conv2(x, edge_index)\n",
    "#         x = F.relu(x)\n",
    "#         # x = self.conv3(x, edge_index)\n",
    "#         # x = F.relu(x)\n",
    "#         x = self.classifier(x)\n",
    "#         # x = F.log_softmax(x, dim=1)\n",
    "#         x = torch.sigmoid(x) \n",
    "#         return x\n",
    "\n",
    "\n",
    "\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(data.num_node_features, 16)\n",
    "        self.conv2 = GCNConv(16, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        output = self.conv2(x, edge_index)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, average_precision_score\n",
    "import torch.nn as nn\n",
    "\n",
    "# Check if a GPU is available, otherwise use CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def train_node_classifier(model, graph, optimizer, criterion, n_epochs=200):\n",
    "    L_train, L_val, ep = [],[], []\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(graph)\n",
    "        loss = criterion(out[graph.train_mask], graph.y[graph.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # pred = out.argmax(dim=1)\n",
    "        acc, val_loss = eval_node_classifier(model, graph, graph.val_mask, criterion)\n",
    "\n",
    "        L_train.append(loss.item())\n",
    "        L_val.append(val_loss)\n",
    "        ep.append(epoch)\n",
    "        if epoch % 10 ==0:\n",
    "            print(f'Epoch: {epoch:03d}, Train Loss: {loss:.3f}, Val Acc: {acc:.3f}')\n",
    "\n",
    "    df = pd.DataFrame({'epoch': ep, 'Train Loss': L_train, 'Val Loss': L_val})\n",
    "    return model, df\n",
    "\n",
    "\n",
    "def eval_node_classifier(model, graph, mask, criterion=nn.CrossEntropyLoss()):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(graph)\n",
    "        loss = criterion(outputs[mask], graph.y[mask])\n",
    "        pred = outputs.argmax(dim=1)\n",
    "        correct = (pred[mask] == graph.y[mask]).sum()\n",
    "        acc = int(correct) / int(mask.sum())\n",
    "    return acc, loss.item()\n",
    "\n",
    "\n",
    "def create_confusion_matrix(predicted, true_labels):\n",
    "    # print(sum(x != y for x, y in zip(predicted, true_labels))/998)\n",
    "    print(confusion_matrix(true_labels, predicted))\n",
    "    print('F1 score = ', f1_score(true_labels, predicted, average='macro'))\n",
    "    print('Precision score = ', precision_score(true_labels, predicted, average='macro'))\n",
    "    print('AUC Precision score = ', average_precision_score(true_labels, predicted, average='macro'))\n",
    "    \n",
    "\n",
    "def plt_performance(df):\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Plot the DataFrame\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(df['epoch'], df['Train Loss'], label='Train Loss')\n",
    "    plt.plot(df['epoch'], df['Val Loss'], label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GlobalStorage' object has no attribute 'train_mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Anaconda3\\envs\\GNNenv\\Lib\\site-packages\\torch_geometric\\data\\storage.py:79\u001b[0m, in \u001b[0;36mBaseStorage.__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 79\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m[key]\n\u001b[0;32m     80\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Anaconda3\\envs\\GNNenv\\Lib\\site-packages\\torch_geometric\\data\\storage.py:104\u001b[0m, in \u001b[0;36mBaseStorage.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, key: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m--> 104\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mapping[key]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'train_mask'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m optimizer_gcn \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(gcn\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m, weight_decay\u001b[39m=\u001b[39m\u001b[39m5e-4\u001b[39m)\n\u001b[0;32m      7\u001b[0m criterion \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m----> 8\u001b[0m gcn, df \u001b[39m=\u001b[39m train_node_classifier(gcn, data, optimizer_gcn, criterion, n_epochs\u001b[39m=\u001b[39;49m\u001b[39m500\u001b[39;49m)\n\u001b[0;32m     10\u001b[0m plt_performance(df)\n\u001b[0;32m     11\u001b[0m \u001b[39m# ----------------------------------------------------------------------------\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[22], line 13\u001b[0m, in \u001b[0;36mtrain_node_classifier\u001b[1;34m(model, graph, optimizer, criterion, n_epochs)\u001b[0m\n\u001b[0;32m     11\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     12\u001b[0m out \u001b[39m=\u001b[39m model(graph)\n\u001b[1;32m---> 13\u001b[0m loss \u001b[39m=\u001b[39m criterion(out[graph\u001b[39m.\u001b[39;49mtrain_mask], graph\u001b[39m.\u001b[39my[graph\u001b[39m.\u001b[39mtrain_mask])\n\u001b[0;32m     14\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     15\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Anaconda3\\envs\\GNNenv\\Lib\\site-packages\\torch_geometric\\data\\data.py:441\u001b[0m, in \u001b[0;36mData.__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m_store\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m:\n\u001b[0;32m    436\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    437\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe \u001b[39m\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m\u001b[39m object was created by an older version of PyG. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    438\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIf this error occurred while loading an already existing \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    439\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdataset, remove the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mprocessed/\u001b[39m\u001b[39m'\u001b[39m\u001b[39m directory in the dataset\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    440\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mroot folder and try again.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 441\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_store, key)\n",
      "File \u001b[1;32mc:\\Anaconda3\\envs\\GNNenv\\Lib\\site-packages\\torch_geometric\\data\\storage.py:81\u001b[0m, in \u001b[0;36mBaseStorage.__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m[key]\n\u001b[0;32m     80\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[0;32m     82\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'GlobalStorage' object has no attribute 'train_mask'"
     ]
    }
   ],
   "source": [
    "data.x = data.x.float()\n",
    "data.y = data.y.long()\n",
    "data = data.to(device)\n",
    "gcn = GCN().to(device)\n",
    "\n",
    "optimizer_gcn = torch.optim.Adam(gcn.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "gcn, df = train_node_classifier(gcn, data, optimizer_gcn, criterion, n_epochs=500)\n",
    "\n",
    "plt_performance(df)\n",
    "# ----------------------------------------------------------------------------\n",
    "pred = gcn(data).argmax(dim=1)\n",
    "\n",
    "pred = pred[data.test_mask] .detach().cpu().numpy()\n",
    "correct = data.y[data.test_mask].detach().cpu().numpy()\n",
    "\n",
    "create_confusion_matrix(pred, correct)\n",
    "\n",
    "\n",
    "test_acc, _ = eval_node_classifier(gcn, data, data.test_mask)\n",
    "print(f'Test Acc: {test_acc:.3f}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
